% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009
\documentclass{acm_proc_article-sp}

\usepackage[utf8]{inputenc}

\begin{document}

\title{Major Language Detection}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
\alignauthor
       Juraj Hreško, Vojtěch Diatka, Kryštof Bořkovec \\
       \affaddr{Seznam.cz, a.s.}\\
       \affaddr{Radlická 3294/10}\\
       \affaddr{Prague 5, Czech Republic}\\
       \email{\{juraj.hresko,vojtech.diatka,krystof.borkovec\}@firma.seznam.cz}
}

\maketitle
\begin{abstract}
TODO
\end{abstract}
\keywords{language detection} % NOT required for Proceedings

\section{Introduction}
  Our main goal was to choose Major Language given on-page language detection results and other features. Finding out languages and their proportion on a webpage is well-established topic in computational processing of web pages (citace). However, determining which of them is major one is not.

  Major Language of a given webpage is the intended language of all potential queries aiming at a given webpage. 

  Intended language is the language of web pages in which a user intends to find results in SERP. This definition implies that if a user searches for a webpages in a language x, our major goal is to provide webpages in language x.
  It is essential topic for any full text search engine to return webpages relevant to a query. One of the dimensions of relevancy is also a language. For each webpage, we thus need to ask: “If the user wanted to find this page, in which language would she formulate the query?” Our interpretation of major language presupposes that user knows what should be the language of the answer to her query.
  This definition strives to exclude any ambiguity. There are very few contexts in which there would be more queries in various languages aiming at the same page.
  We approached this topic with a machine-learning method based on our implementation of multiple additive oblivious decision trees called RC-rank.


  Our paper has X sections:
  \begin{enumerate}
  \item Introduction
  \item Task (Popis úlohy – kolik jazyků a které (a proč tyto jazyky), na jakou množinu aplikujeme klasifikátor, jaký je poměr jazyků tam…..)

  \item Related work (problem se neřeší tak často, ale zakládáme se na onpage detekci)

  \item Classifier  (co je zač a jak jsme si vyhráli s fíčurama)
  \item Feature description
  \item Training set
  \item Results
  \item Comparison with other majorLang algorithms
  \end{enumerate}
  
\section{Related Work}

  To our knowledge, there is no paper dealing with choosing the main language from the set
  of languages identified on a webpage. Except for a few remarks on this topic in papers 
  focusing on on-page language detection this topic seems to be unexplored. 
  Necessary prerequisite for our research is on-page detection. Many researchers approached this topic
  from various perspectives. For a comparison of some of these models, see \cite{Baldwin:shortlong}. 
  First algorithms were based on n-grams comparing frequencies of n-grams in a given document with 
  language models - frequency list of n-grams \cite{trenkle:ngram}. Current algorithms classifying languages 
  are based on Support Vector Machines (SVM). System based on SVM first maps input into a high dimensional space. 
  Then, it separates classes with a hyperplane (\cite{Campbell:supportvector}, \cite{Lodhi:textclass}).
  Our on-page identification was based on an algorithm by Řehůřek and Kolkus (\cite{Rehurek:languageidentification}).  They elaborated the n-gram approach. 
  “We propose and evaluate a new method, which constructs language models based on word relevance. We also 
  extend our method to allow us to efficiently and automatically segment the input text into blocks of individual 
  languages, in case of multiple-language documents.” (\cite{Rehurek:languageidentification})
  We would like to mention one example of how other researchers have dealt with our task. As it was not the main goal of 
  their paper, their solution is very simple:
  “In these cases, we try to re-apply the n-gram algorithm, weighting the largest continuous text block in the document 
  (blocks are identied in the HTML parsing stage, taking in account the markup information) as three times more important 
  than the normal text. The rationale for this is that the longest block will very likely correspond to a good description 
  of the page, possibly in its main language.” (\cite{Martins:langidentweb}) If we are to determine the major language of a website 
  we can’t confine ourselves to choose simply the language with highest proportion. A good example of this is a case of an e-shop 
  containing 80\% English and 20\% Czech. Despite this language proportion we would like to choose Czech as the main language. 
  English on this webpage comes up only as the names of products in a very long list. Only the menu is Czech. In this case 
  we need to assign this page a Czech main language tag because we want to have it as a landing page for queries in Czech.

\section{Problem description}

  For our purpose we needed to work with specific categories of languages.
  As a web fulltext search company we are working with non-trivial volume of web documents.
  Due to technical limitations as well as because our services target mainly market in the Czech Republic,
  our database of indexed documents represents sample with specific language distribution which is different from the
  language distribution of all documents in the Internet.
  The distribution is based primarily on the list of “main” languages –
  Czech, English, Slovak, German, Polish and French –
  selected as most interesting latin-alphabet languages for local users and representing  our first six classes.
  As we are not trying to differentiate other languages, we created three other language classes specified only by writing system
  – latin, cyrillic and other (e.g. in order - Hungarian, Bulgarian and Armenian language).

  \begin{table}[]
 \centering
 \caption{Languages distribution in dataset - fixme} 
 \label{langdist} 
 \vspace{0.5cm}
 \begin{tabular}{l||c}
    Czech & 61.9\% \\
    English & 19.1\% \\
    Slovak & 3.0\% \\
    German & 1.7\% \\
    Polish & 0.7\% \\
    French & 0.6\% \\ \hline
    Latin & 7.7\% \\
    Cyrillic & 3.2\% \\
    Other & 3.1\% \\
 \end{tabular}
 \end{table}


  \section{Solution}

  Our task resembles a multi-category classification problem. 
  However this approach brings a number of issues arising primarily from the fact of class multiplicity. 
  In the first place, there is a problem with sampling as we needed to keep corresponding proportions 
  of real distribution of languages across our dataset. 
  This leads to the situation when some classes examples are quite rare and so their count is insufficient 
  for training proper classifier. We handled this aspect of the task by joining all positive samples 
  into one class and modifying the features by creating a special group of so called \textit{generic language features}. 
  Using this approach we were able to create more general classifier, which works as a ranker of all considered languages. 
  The final model was trained using RC-Rank algorithm which is based on multiple additive oblivious decision trees \cite{maimon:datamining}. 

  \subsection{Data}

  We have started by annotating 3,456 web pages randomly selected from database
  used by Seznam.cz's full-text search engine. Each URL was manually assigned
  by the authors to exactly one of 10 following language classes {\texttt{CS}, \texttt{DE},
  \texttt{EN}, \texttt{FR}, \texttt{PL}, \texttt{SK}, \texttt{UNKNOWN}, 
  \texttt{UNKNOWN\_CYRILLIC}, \texttt{UNKNOWN\_LATIN},
  \texttt{UNKNOWN\_OTHER}. Documents in the created set (represented by their
  URLs) were subsequently divided into two separate sets for training and
  testing. Our training set consisted of 2,410 web pages and our testing set
  contained the remaining 1,046 web pages. As soon as we realized that some
  language classes are obviously not sufficiently represented in our training
  data, we used the above-described trick (of constructing a single classifier for
  ten languages instead of ten different classifiers) to inflate the data. Thus
  we generated ten training instances for each annotated URL resulting in final
  sizes of 24,100 and 10,460 web pages for training and testing set respectively.


  \subsection{Features}
  Our classifier takes advantage of a total of 143 features (not including additional 96 mixed features) divided into four groups
  as follows: \textit{language-specific features}, \textit{generic language features}, \textit{non-language
  features} and \textit{mixed features}. In this section we briefly characterize each of these
  groups.

\subsubsection{Language-specific features}

  The classifier employs 110 language-specific features falling into 11 different trait groups. Within each group a
  particular trait of a site is examined for each of 10 language classes separately. Selection of
  inspected language traits follows: 
  \begin{itemize}
    \item \textit{on-page language detection} - E.g. feature named \texttt{on\_page\_CS} determines the
      probability of a web page being in the Czech language based on analysis of all the text
      contained on the page being scrutinized. Similarly \texttt{on\_page\_DE} determines the
      probability for German language etc. Besides the above-described all-text analysis, the \textit{on-page 
      detector} is also used for analysing language of \textit{site-wide} text part (\texttt{SWT}) and
    \textit{non-site-wide} text (\texttt{non-SWT}) part of the document separately as site-wide texts 
      such as menus may tend to be in different language than site-specific texts such as articles' bodies.
    \item \textit{backlink language prediction} - probabilities of the page being in a particular
      language based on language characteristics of other pages referencing it (e.g.
      \texttt{backlink\_EN})
    \item \textit{language statistics aggregated for domains} - language statistics for the
      containing domains of different levels
    \item \textit{language specifications in} HTML - such as \texttt{<HTML lang=...>} or \texttt{<HTML
      meta http-equiv="Content-Language"  ...>}
    \item \text{selected language indicative tokens in URL} such as \texttt{cs} in:

    \texttt{http://www.europarl.europa.eu/news/}\underline{\texttt{cs}} 
    \item HTTP \textit{header field} \texttt{Content-language}
  \end{itemize}


 \subsubsection{Generic language features}
    For feature generation purposes, one can examine each web page from 10 different perspecitves -
    each corresponding to one particular language (i.e. a class). With generic language features, we
    take advantage of this fact by shifting from asking questions such as \textit{what is the probability that 
    this particular page is in the Czech language based on backlink statistics?} to questions 
    such as \textit{what is the probability that this page is in the language we are currently checking 
    based on backlink statistics?} Technically, this means simply
    that for each \textit{view} (i.e. particular language), we copy values of language-specific
    features for that language and use them as generic language features for the current view. Thus
    we end up with 11 generic language features (one for each trait group of language-specific
    features) such as \texttt{on\_page\_lang}, \texttt{backlink\_lang}, etc.
 \subsubsection{Non-language features}
    Not all used features are directly related to document language. Features in this group may help the
    classifier to first split the data space into distinct categories exhibiting different data patterns 
    to subsequently enhance the language detection itself. 

    \begin{itemize}
      \item \textit{type of site} - probabilities of the site being an e-shop, photo gallery etc.
      \item \textit{prices and currencies} - total numbers of matches of a set of regular
      expressions designed to detect prices in particular currencies (e.g. \texttt{\$35}) 
      \item \textit{text length} 
      \item \textit{backlink count}
    \end{itemize}
    
  \subsubsection{Mixed features}

    To make the job easier for the classifier, we also used mixed features which combine previously
    described features in various ways. Each of mixed features is either a sum or a product of a
    pair or a triplet of some selected features.


  \section{Evaluation}
  \section{Discussion}
\bibliographystyle{abbrv}
\bibliography{major_lang_paper}

\balancecolumns

% That's all folks!
\end{document}
