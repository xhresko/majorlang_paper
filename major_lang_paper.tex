% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009
\documentclass{acm_proc_article-sp}

\usepackage[utf8]{inputenc}
\begin{document}

\title{Major Language Detection}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
%\alignauthor
%       Juraj Hreško, Vojtěch Diatka, Kryštof Bořkovec \\
%       \affaddr{Seznam.cz, a.s.}\\
%       \affaddr{Radlická 3294/10}\\
%       \affaddr{Prague 5, Czech Republic}\\
%       \email{\{juraj.hresko,vojtech.diatka,krystof.borkovec\}@firma.seznam.cz}
}

\maketitle
\begin{abstract}
The major language of a given website is the intended language of all potential queries aiming at a given webpage.
For any search engine, it is beneficial to know the major language of a site, since it is easier to match
queries with corresponding webpages.

Czech search engine company Seznam.cz used to approach this task with handcrafted algorithm with F1 measure reaching
90.8 percent. After applying machine-learning method based on our implementation of multiple additive oblivious decision trees
to this task F1 has risen to 97.8 percent, which translates to improving major language detection for tenths of millions of webpages.

This work shows how the simple reformulation of this specific multi-class classification task could help
to create more general model usable even for quite rare classes with only few training samples available.


\end{abstract}
\keywords{language detection, multi-class classification, information retrieval} % NOT required for Proceedings

\section{Introduction}
  Delivering the content based on the preferences of the user is one of the basic tasks of information retrieval systems.
  The most important content parameter is called relevance. 
  However, this parameter somehow inherently contains other parameters, which can be called usability.
  Usability can be defined as a probability that user will be able to use the content of particular 
  web page to satisfy his (intended) needs.
  Usability of the page is not solely a parameter of a page itself.
  It could be said that it is a parameter of the user - web page relation.

  Language of a web page affects the usability with different intensity. 
  The weight of its influence depends on the type of page content.
  For example, there is almost no need for understanding the language of the page, if it provides mainly images or video content.
  Web page readability is telling us, if particular user is able to
  ``use'' the text on the web page for his intent (e.g. reading a joke, comments or navigation elements).
 

  The readability could be easily measured, if we know that the page contains text written in only one language,
  and if we know the languages known by the user.
  E.g. it can be said that a page written solely in a language $L_a$ would not be readable for a reader 
  that does not know the language $L_a$. 
  In this case, (when there is only one language used on the page) 
  the so called ``major'' language of the page is $L_a$, further on denoted by $L_{maj} = L_a$.
  
  However, there are many web pages that contain more than one language.
  Finding out languages and their proportion on a webpage is a well-established topic in computational processing of 
  web pages \cite{ Baldwin:shortlong, Campbell:supportvector, trenkle:ngram, Martins:langidentweb, Rehurek:languageidentification}.
  It could be harder to decide which language would be the ``major'' for these.
 
  \section{Major language}
  
  ``Major'' language ($L_{maj}$) of a given webpage is the intended language of all potential queries aiming at a given webpage.
  Intended language is the language of web pages in which a user intends to find results in SERP. 
  This definition implies that if a user searches for webpages in a language $L_a$, 
  our major goal is to provide webpages for which $L_{maj} = L_a$.
  
  For each webpage we thus need to ask: “If the user wanted to find this page, in which language would he formulate the query?” 
  Our interpretation of the major language presupposes that the user knows what should be the language of the answer to his query.

  To choose the language $L_{maj}$ features such as the presence of chunks of texts written in a particular language or statistics based on 
  backlink language predition were used. 
  We approached this topic with a machine-learning method based on our implementation of multiple additive oblivious decision trees called RC-rank (see section \ref{sec:classifier}).

\section{Related Work}

  To our knowledge, there is no paper dealing with choosing the main language from the set
  of languages identified on a webpage. Except for a few remarks on this topic in papers 
  focusing on on-page language detection this topic seems to be unexplored. 
  
  A necessary prerequisite for our research is an on-page language detection. Many researchers approached this topic
  from various perspectives. For a comparison of some of these models, see \cite{Baldwin:shortlong}. 
  First algorithms were based on n-grams comparing frequencies of n-grams in a given web page with 
  language models - frequency list of n-grams \cite{trenkle:ngram}. Current algorithms classifying languages 
  are based on Support Vector Machines (SVM). A system based on SVM first maps input into a high dimensional space. 
  Then, it separates classes with a hyperplane \cite{Campbell:supportvector, Lodhi:textclass}.
  Our on-page identification was based on an algorithm by Řehůřek and Kolkus \cite{Rehurek:languageidentification}.  They elaborated the n-gram approach. 
  “We propose and evaluate a new method, which constructs language models based on word relevance. We also 
  extend our method to allow us to efficiently and automatically segment the input text into blocks of individual 
  languages, in case of multiple-language web pages.” \cite{Rehurek:languageidentification}
  
  
  There is one example of how other researchers have dealt with our task. As it was not the main goal of 
  their paper, their solution was very simple. It can be labeled as a majority rule approach:
  “In these cases, we try to re-apply the n-gram algorithm, weighting the largest continuous text block in the page 
  (blocks are identified in the HTML parsing stage, taking into account the markup information) as three times more important 
  than the normal text. The rationale for this is that the longest block will very likely correspond to a good description 
  of the page, possibly in its main language.” \cite{Martins:langidentweb} If the task is to determine the major language of a website 
  the language with the highest proportion can not be the only criterion. A good example of this is a case of a webpage of an online store
  containing 80\% English and 20\% Czech. Despite this language proportion it is right to choose Czech as the main language
  English on this webpage comes up only as the names of products in a very long list. Only the navigational elements are labeled in Czech. In this case 
  a Czech main language tag should be assigned to this page because this should be a landing page for queries in Czech.

\section{Languages}

  For our purpose we needed to work with specific categories of languages.
  Fulltext search company Seznam (we work for) works with non-trivial volume of web pages.
  Due to technical limitations, as well as because our services target mainly market in the Czech Republic,
  our database of indexed pages represents sample with specific language distribution (Table \ref{langdist}) which is different from the
  language distribution of all web pages in the Internet.
  The distribution is based primarily on the list of ``preferred'' languages –
  Czech, English, Slovak, German, Polish and French –
  selected as the most interesting latin-alphabet languages for local users and representing  our first six classes.
  As no other languages were differentiated, three other language classes were created. They were specified only by writing system
  – latin, cyrillic and other (e.g. Hungarian, Bulgarian and Armenian language, respectively).

  \begin{table}[]
 \centering
 \caption{Languages distribution in dataset} 
 \label{langdist} 
 \vspace{0.2cm}
 \begin{tabular}{l||c}
    Czech & 61.9\% \\
    English & 19.1\% \\
    Slovak & 3.0\% \\
    German & 1.7\% \\
    Polish & 0.7\% \\
    French & 0.6\% \\ \hline
    Latin & 7.7\% \\
    Cyrillic & 3.2\% \\
    Other & 3.1\% \\
 \end{tabular}
 \end{table}


 \section{Classifier}
 \label{sec:classifier}

  Our task resembles a multi-class classification problem. 
  This approach brings a number of issues arising primarily from the fact of class multiplicity. 
  In the first place, there is a problem with sampling as it was necessary to keep corresponding proportions 
  of real distribution of languages across our dataset. 
  This leads to the situation when some classes examples are quite rare and so their count is insufficient 
  to train proper classifier. Also, this approach misses one generalisation valid for our data.
  It is assumed that all language classes behave uniformly with respect to feature values. In other words, 
  one combination of feature values based on presence of English leads to classifying the example as English, whereas 
  the same or similar combination of feature values based on presence of Czech leads to classifying an example as Czech.
  In the multi-category classification, this generalisation is lost because all language-specific features are considered
  not to behave in the same/similar way.

  The solution for this problem is offered by re-formulating our task.
  Apart from language specific features, such as proportion of the text on a given 
  web page, a \textit{generic language features} were included in feature vectors. These features work as follows:
  Our classifier looks at each example from $n$ perspectives, where $n$ is the number of language classes.
  The principle is illustrated in Figure 1. 
  In a multi-class classification task (A) there is a sample with specific label (e.g. $L2$).
  It would be represented by language-specific features (for each language $L_1,L_2,L_3$ ) and some 
  language independent features (Other).


  Sample for each language is created with re-formulation (B).
  Generic language features of this language $L_i$ are used and all the language-specicific
  ones ($L_1, L_2, L_3$). The class label indicate whether the given example is in language $L_i$ (1) or not (0).


  With generic language features, we are shifting from asking questions such as \textit{what is the probability that 
    this particular page is in the Czech language based on backlink statistics?} to questions 
    such as \textit{what is the probability that this page is in the language we are currently checking 
    based on the backlink statistics?} 
 
 Each example was viewed from the perspective of all nine languages. The 
  language-specific features remained the same for each nine clones of one example,
  but generic language features took values of the language whose perspective
  it was analyzed from.  Thus, eight false training instances and 
  one true are generated, in case generic language features were set properly according to 
  language of manual annotation.

  \begin{figure}
      \caption{Multi-class vs. Ranking} 
     \label{examples} 
 
 \vspace{0.2cm}
      \centering
      \includegraphics[scale=0.5]{multi-class.eps}
 \end{figure}

  The nine samples for any given URL are created during the evaluation phase
  and the classifier is used to rank them, using the probability of a possitive label as rank value.
  Consequently, the top sample from the nine has been chosen and the final label was predicted as $L_i$ language of this sample.

  Using this approach, it was possible to create a more general classifier which works as a ranker of all considered languages.
  The final model was trained using the RC-Rank algorithm, which is the internal algorithm of Seznam.cz suitable for both, classification and ranking.
  It is using boosted regression trees like MART algorithm (multiple additive regression trees) \cite{friedman:mart}. 
  However, it is using a few additional regularization techniques to prevent overfitting even for small datasets.
  The most important is the usage of “oblivious trees” (described in \cite{langley:oblivious}). 
  It means that the search for optimal threshold is not optimizing some function (e.g. MSE) just for one node, 
  but for all nodes on the same level in the tree. 
  This approach is similar to the one which is used in MatrixNet from Yandex used in \cite{trofimov:matrixnet}.



  \section{Training data}

  We used approximately 3,800 \footnote{We started by annotating over 4,000 pages.
  Unfortunately, all signals for all of them were not acquired for technical reasons and thus
  all the data were not used in the end.} manually annotated 
  web pages randomly selected from database
  used by Seznam.cz's full-text search engine. Each URL was manually assigned
  to exactly one of nine following language classes {\texttt{CS}, \texttt{DE},
  \texttt{EN}, \texttt{FR}, \texttt{PL}, \texttt{SK},  
  \texttt{UNKNOWN\_CYRILLIC}, \texttt{UNKNOWN\_LATIN},
  \texttt{UNKNOWN\_OTHER}. 
  
  Web pages in the created set (represented by their
  URLs) were subsequently divided into two separate sets for training and
  testing. Our training set consisted of 1,918 web pages and our testing set
  contained the remaining 1,866 web pages. 
  As some language classes were not sufficiently represented in our training
  data, the above-described trick was used ( constructing a single classifier for
  nine languages instead of nine different classifiers) to expand the data set.
  The final size of our datasets was 17,262 and 16,794 samples for training and testing set respectively.


  \subsection{Features}
  Our classifier takes advantage of a total of 143 features (not including additional 96 mixed features) divided into four groups
  as follows: \textit{language-specific features}, \textit{generic language features}, \textit{non-language
  features} and \textit{mixed features}. Each of these
  groups are briefly characterized in this section.

\subsubsection{Language-specific features}

  The classifier employs 110 language-specific features falling into 11 different  groups. Within each group a
  particular trait of a site is examined for each of the nine language classes separately. Selection of
  inspected language traits follows: 
  \begin{itemize}
    \item \textit{on-page language detection} - 
      %E.g. feature named \texttt{on\_page\_CS} 
      determines the probability of a web page being written in some particular
      %in the Czech 
      language based on the analysis of all the text
      contained on the page with on-page detector based on \cite{Rehurek:languageidentification} . 
      %Similarly \texttt{on\_page\_DE} determines the
      %probability for German language etc. 
      Besides the above-described analysis of the whole text, the \textit{on-page 
      detector} is also used for analysing language of \textit{site-wide} text (\texttt{SWT}) and
    \textit{non-site-wide} text (\texttt{non-SWT}) separately. Site-wide text is a textual html element 
    present on all pages from a give site such as menus. The reason for that is that they might be 
    in different language than site-specific texts such as articles' bodies.
    \item \textit{backlink language prediction} - probabilities of the page being in a particular
      language based on the language characteristics of other pages referencing to it 
      %(e.g.\texttt{backlink\_EN})
    \item \textit{language statistics aggregated for domains} - language statistics for the
      containing domains of different levels
    \item \textit{language specifications in} HTML - such as \texttt{<HTML lang=...>} or \texttt{<HTML
      meta http-equiv="Content-Language"  ...>}
    \item \text{selected language indicative tokens in URL} such as \texttt{cs} in:

    \texttt{http://www.europarl.europa.eu/news/}\underline{\texttt{cs}} 
    \item HTTP \textit{header field} \texttt{Content-language}
  \end{itemize}


 \subsubsection{Generic language features}
    For feature generation purposes, one can examine each web page from nine different perspecitves -
    each corresponding to one particular language (i.e. a class).
   Technically, this simply means 
    that for each \textit{view} (i.e. particular language), values of language-specific
    features for that language are copied and are used as the generic language features for the current view. Thus
    there are 11 generic language features (one for each trait group of language-specific
    features) such as \texttt{on\_page\_lang}, \texttt{backlink\_lang}, etc.
 \subsubsection{Non-language features}
    Not all used features are directly related to the web page language. Features in this group may help the
    classifier to first split the data space into distinct categories exhibiting different data patterns 
    to subsequently enhance the language detection itself.

    \begin{itemize}
      \item \textit{type of site} - probabilities of the site being an online store, image gallery etc.
      Our classifier trained to recognize such web sites was used.
      \item \textit{prices and currencies} - total numbers of matches of a set of regular
      expressions designed to detect prices in particular currencies (e.g. \texttt{\$35}) 
      \item \textit{text length} - defined as a number of tokens
      \item \textit{backlink count} - this feature was used in order to weight the influence of feature
      back-link language prediction.
    \end{itemize}

  \subsubsection{Mixed features}
    To make the job easier for the classifier, mixed features are also used. They combine previously
    described features in various ways. Each of mixed features is either a sum or a product of a
    pair or a triplet of some selected features. 

    \section{Evaluation}
 For the evaluation purposes, we decided to compare the proposed approach with the simple ``majority rule approach'' \cite{Martins:langidentweb} as we were 
 not able to find other solutions dedicated to this particular task. It basically states that the language with the highest proportion on a webpage
 is the major language. Approximately 1,900 training samples were used to create model. 
 The rest of the labeled data - approximately 1,900 examples - was used as testing dataset.
 The final model consists of 150 trees with depth 5.
 

% \begin{table}[]
% \centering
% \caption{Evaluation results (in percents)}
% \label{evalres}
% \vspace{0.5cm}
% \begin{tabular}{l||c|c|c||c|c|c}
%       &
%     \multicolumn{3}{|c|}{\textbf{Majority vote}} &
%     \multicolumn{3}{|c}{\textbf{Our approach}} \\ \hline
%     \textbf{Lang.}    & \textbf{Prec.}  & \textbf{Rec.}   & \textbf{F1}    & \textbf{Prec.}  & \textbf{Rec.}   &  \textbf{F1}    \\ \hline
%     \textbf{Czech}    & 98.56  & 90.84  & 94.54 & 99.11  & 98.82  & 98.96  \\
%     \textbf{English}  & 86.67  & 97.76  & 91.88 & 96.78  & 99.21  & 97.98  \\
%     \textbf{Slovak}   & 97.10  & 80.72  & 88.16 & 98.80  & 98.80  & 98.80  \\
%     \textbf{German}   & 73.68  & 91.80  & 81.75 & 95.08  & 95.08  & 95.08  \\
%     \textbf{Polish}   & 81.82  & 94.74  & 87.80 & 100.00 & 100.00 & 100.00 \\
%     \textbf{French}   & 88.24  & 100.00 & 93.75 & 100.00 & 100.00 & 100.00 \\ \hline
%     \textbf{Latin}    & 85.71  & 61.94  & 71.91 & 96.50  & 89.03  & 92.62  \\
%     \textbf{Cyrillic} & 100.00 & 90.00  & 94.74 & 100.00 & 93.33  & 96.55  \\
%     \textbf{Other}    & 100.00 & 88.46  & 93.88 & 100.00 & 94.23  & 97.03  \\ \hline \hline
%     \textbf{W. sum.}  & 91.51  & 90.94  & 90.78 & 97.86  & 97.86  & 97.84  \\
% \end{tabular}
% \end{table}


\begin{table}[]
 \centering
 \caption{Evaluation results (in percents)}
 \label{evalres}
 \vspace{0.2cm}
 \begin{tabular}{l||c|c|c|}
       &
     \multicolumn{3}{|c}{\textbf{Majority rule approach}} \\ \hline
     \textbf{Lang.}    & \textbf{Prec.}  & \textbf{Rec.}   & \textbf{F1}  \\ \hline
     \textbf{Czech}    & 98.56  & 90.84  & 94.54 \\
     \textbf{English}  & 86.67  & 97.76  & 91.88 \\
     \textbf{Slovak}   & 97.10  & 80.72  & 88.16 \\
     \textbf{German}   & 73.68  & 91.80  & 81.75 \\
     \textbf{Polish}   & 81.82  & 94.74  & 87.80 \\
     \textbf{French}   & 88.24  & 100.00 & 93.75 \\ \hline
     \textbf{Latin}    & 85.71  & 61.94  & 71.91 \\
     \textbf{Cyrillic} & 100.00 & 90.00  & 94.74 \\
     \textbf{Other}    & 100.00 & 88.46  & 93.88 \\ \hline \hline
     \textbf{W. sum.}  & 91.51  & 90.94  & 90.78 \\
 \end{tabular}
 \end{table}


 \begin{table}[]
 \centering
 \caption{Evaluation results (in percents)}
 \label{evalres}
 \vspace{0.2cm}
 \begin{tabular}{l||c|c|c|}
       &
     \multicolumn{3}{|c}{\textbf{Our approach}} \\ \hline
     \textbf{Lang.}    & \textbf{Prec.}  & \textbf{Rec.}   & \textbf{F1} \\ \hline
     \textbf{Czech}    & 99.11  & 98.82  & 98.96  \\
     \textbf{English}  & 96.78  & 99.21  & 97.98  \\
     \textbf{Slovak}   & 98.80  & 98.80  & 98.80  \\
     \textbf{German}   & 95.08  & 95.08  & 95.08  \\
     \textbf{Polish}   & 100.00 & 100.00 & 100.00 \\
     \textbf{French}   & 100.00 & 100.00 & 100.00 \\ \hline
     \textbf{Latin}    & 96.50  & 89.03  & 92.62  \\
     \textbf{Cyrillic} & 100.00 & 93.33  & 96.55  \\
     \textbf{Other}    & 100.00 & 94.23  & 97.03  \\ \hline \hline
     \textbf{W. sum.}  & 97.86  & 97.86  & 97.84  \\
 \end{tabular}
 \end{table}

 It can be observed in the result table, that the simple majority rule F1 score was improved in average by 7\% (weighted average for all languages).
 The difference ranges from 10\% - 13\% for Slovak and German to smaller improvements above 4\% for Czech language.
 However, not all the results for particular languages are so reliable, as there were only few pages representing some of the languages.
 This is obvious especially for Polish and French languages,  where there were only 19, respectively 30 web pages present 
 (another two smaller language groups were Cyrillic and Other).


 \section{Conclusions}
 Seznam.cz used to detect the major language of a website by hand crafted algorithm
 employing a decision tree with if conditions branches. Applying the above-mentioned machine learning algorithm
 to this task brought Seznam.cz a significant improvement in precision and recall in language detection.

 Determining the major language of a webpage is very important topic for Seznam.cz because it 
 influences relevance of the whole SERP and thus users experience with the search engine. For queries in
 language $L$, search engine should return webpages in language $L$. To perform this task well, search engine
  needs to know the major language of each website. The new classifier has been implemented
  and is currently used in Seznam.cz to date. It has been applied to all webpages in our database,
 which currently ammounts to approximately 1.5G entries. 
 
 
 
 To make the above-mentioned algorhitm perform even better, we see an opportunity in the web page type
 classifier which classifies webpages into groups, such as online store, image gallery, article etc.
 If it works with higher precision, it would be possible to set aside 
 difficult web pages such as online stores which still pose a problem for the new classifier.
 They could then be treated differently by the new classifier to achieve better results.
\bibliographystyle{abbrv}
\bibliography{major_lang_paper}

\balancecolumns

% That's all folks!
\end{document}
